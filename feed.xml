<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="/taobataocp/feed.xml" rel="self" type="application/atom+xml" /><link href="/taobataocp/" rel="alternate" type="text/html" /><updated>2021-02-23T05:47:01+00:00</updated><id>/taobataocp/feed.xml</id><title type="html">The Art of Blogging About “The Art of Computer Programming”</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">section-1.3.3</title><link href="/taobataocp/taobataocp/2021/02/23/section-1.3.3.html" rel="alternate" type="text/html" title="section-1.3.3" /><published>2021-02-23T05:36:35+00:00</published><updated>2021-02-23T05:36:35+00:00</updated><id>/taobataocp/taobataocp/2021/02/23/section-1.3.3</id><content type="html" xml:base="/taobataocp/taobataocp/2021/02/23/section-1.3.3.html">&lt;p&gt;This section talks about permutations.  The key parts are 2 algorithms for permutation composition (Knuth calls it multiplication).  One algorithm is pretty human-like with repeated scans over the input, and the other is single pass and more “machine like.”  There’s assembly code for both which has passing resemblance to modern assembly, and honestly seems to have little value.  There’s really no need to examine code that has a loop to read input data from punch cards.&lt;/p&gt;

&lt;p&gt;The analysis has a discussion of “kirchoff’s law” for statements, which I think I mentioned before.  I think I also mentioned that doing precise counts of every instruction seems somewhat useless these days given that 1 cache miss will take more time than 100 instructions.  There are a couple of clever algorithms for computing inverse permutations which are unfotunately tricky to follow/understand.  The pseudocode for the algorithms is really hard to understand since because it’s written in a prose with a goto style.&lt;/p&gt;

&lt;p&gt;I should maybe chalk it up to being in a bad mood because I’ve been dealing with cheating students this evening, but I found myself more annoyed with how antiquated the presentation is than appreciative of the insights/ideas.  Also, I recognize the irony of being irritated by a book from the 1970s about computers being out of date.&lt;/p&gt;</content><author><name></name></author><category term="taobataocp" /><summary type="html">This section talks about permutations. The key parts are 2 algorithms for permutation composition (Knuth calls it multiplication). One algorithm is pretty human-like with repeated scans over the input, and the other is single pass and more “machine like.” There’s assembly code for both which has passing resemblance to modern assembly, and honestly seems to have little value. There’s really no need to examine code that has a loop to read input data from punch cards.</summary></entry><entry><title type="html">section-1.3.2</title><link href="/taobataocp/taobataocp/2021/01/28/section-1.3.2.html" rel="alternate" type="text/html" title="section-1.3.2" /><published>2021-01-28T06:07:50+00:00</published><updated>2021-01-28T06:07:50+00:00</updated><id>/taobataocp/taobataocp/2021/01/28/section-1.3.2</id><content type="html" xml:base="/taobataocp/taobataocp/2021/01/28/section-1.3.2.html">&lt;p&gt;This section is about MIX Assembly Language (MIXAL)… It seemed to age even worse to me than the MIX architecture.&lt;/p&gt;

&lt;p&gt;I realized through an example in this section that the call stack wasn’t a normal thing, or at least MIX doesn’t have call/ret instructions.  On a jump, the PC +1 is stored in a special register, and the callee is responsible for overwriting the “return” instruction (just a plain old jump) to set it’s address field to the caller’s PC +1.  There’s a name for this from a computerphile video I watched a while back, but can’t remember it.  In any case, thank god for call, ret, and a standardized understanding of how a stack should work.&lt;/p&gt;

&lt;p&gt;Perhaps part of the reason that I found this section to have aged so poorly is that there’s a couple of pages devoted to how to write out MIXAL on a punch card.  I’m hoping the rest of the book focuses enough on the algorithms/analysis that the idosyncrasies of a fictional 50+ year old computer design don’t weigh too heavily.&lt;/p&gt;</content><author><name></name></author><category term="taobataocp" /><summary type="html">This section is about MIX Assembly Language (MIXAL)… It seemed to age even worse to me than the MIX architecture.</summary></entry><entry><title type="html">section-1.3.1</title><link href="/taobataocp/taobataocp/2021/01/27/section-1.3.1.html" rel="alternate" type="text/html" title="section-1.3.1" /><published>2021-01-27T05:16:18+00:00</published><updated>2021-01-27T05:16:18+00:00</updated><id>/taobataocp/taobataocp/2021/01/27/section-1.3.1</id><content type="html" xml:base="/taobataocp/taobataocp/2021/01/27/section-1.3.1.html">&lt;p&gt;Finally we get to MIX.  There’s a note in my edition, which I think was from 1997, that MIX is woefully out of date and will be replaced by MMIX (2009) which will be a normal-seeming 64 bit machine.  Apparently a revised volume 1 came out in 2005 which uses MMIX.&lt;/p&gt;

&lt;p&gt;The word format takes up a good chunk of the description with words being 5 “bytes” + a sign bit.  A “byte” stores numbers from 0 to something between  64 and 100.  You’re not supposed to know the exact size of a byte and so doing operations on pieces of a word have to be performed very carefully.  It will be interesting to see how much advantage of this is taken later on to do weird data packings, etc.&lt;/p&gt;

&lt;p&gt;The instructions are relatively normal with the exception of the addressing modes + weird word size.  There are 6 index registers which can be referenced as part of an instruction to add an offset to an address.  All instructions take a memory address, so I don’t think there are actually any register/register operations.  I guess some of the rotate/shift instructions don’t touch memory?&lt;/p&gt;

&lt;p&gt;This chapter is definitely a reminder of how much has changed in 50 years, both in the design of MIX and the lack of standardization of real machines, and in the timing estimates (memory and compute take equal time).&lt;/p&gt;</content><author><name></name></author><category term="taobataocp" /><summary type="html">Finally we get to MIX. There’s a note in my edition, which I think was from 1997, that MIX is woefully out of date and will be replaced by MMIX (2009) which will be a normal-seeming 64 bit machine. Apparently a revised volume 1 came out in 2005 which uses MMIX.</summary></entry><entry><title type="html">section-1.2.11.2And3</title><link href="/taobataocp/taobataocp/2021/01/17/section-1.2.11.2And3.html" rel="alternate" type="text/html" title="section-1.2.11.2And3" /><published>2021-01-17T23:34:46+00:00</published><updated>2021-01-17T23:34:46+00:00</updated><id>/taobataocp/taobataocp/2021/01/17/section-1.2.11.2And3</id><content type="html" xml:base="/taobataocp/taobataocp/2021/01/17/section-1.2.11.2And3.html">&lt;p&gt;The first subsection talks about an approximation due to Euler for computing sums using integrals.  The Bernulli polynomials/coefficients make an appearance here.  There’s lots of integral and summation manipulation that takes place here.  I mostly skimmed this section.&lt;/p&gt;

&lt;p&gt;The one interesting piece was that at one point, we have to come up for a formula for the mth derivative of f(x).  This seemed like a math example of a “higher order function” or metaprogramming or some other high-level functional abstraction.&lt;/p&gt;

&lt;p&gt;The second subsection was some proofs and approximations related to a couple of series… I didn’t even read through the whole section.  Next section is MIX, finally.&lt;/p&gt;</content><author><name></name></author><category term="taobataocp" /><summary type="html">The first subsection talks about an approximation due to Euler for computing sums using integrals. The Bernulli polynomials/coefficients make an appearance here. There’s lots of integral and summation manipulation that takes place here. I mostly skimmed this section.</summary></entry><entry><title type="html">section-1.2.11</title><link href="/taobataocp/taobataocp/2021/01/15/section-1.2.11.html" rel="alternate" type="text/html" title="section-1.2.11" /><published>2021-01-15T03:36:50+00:00</published><updated>2021-01-15T03:36:50+00:00</updated><id>/taobataocp/taobataocp/2021/01/15/section-1.2.11</id><content type="html" xml:base="/taobataocp/taobataocp/2021/01/15/section-1.2.11.html">&lt;p&gt;This section is about big O notation.  It’s more or less the standard intro you’d get in a ugrad algorithms class.  What’s very interesting to me is that it’s marked as a “skip the first time you read this” section because “it covers a rather specialized topic that is interesting but not essential.”  I would say that big O analysis has since become essential.  Based on what I’ve read so far, and what I recall from the last time I read part of the book(s) is that the analysis is very exact and all coefficients and constants are carefully computed.  Given how unequal different operations are (a multiply vs a read from L2 cache vs a network round trip), it seems that counting operations is probably not worth the trouble in almost all cases.  It will be interesting to see how Big O is used throughout the book.&lt;/p&gt;</content><author><name></name></author><category term="taobataocp" /><summary type="html">This section is about big O notation. It’s more or less the standard intro you’d get in a ugrad algorithms class. What’s very interesting to me is that it’s marked as a “skip the first time you read this” section because “it covers a rather specialized topic that is interesting but not essential.” I would say that big O analysis has since become essential. Based on what I’ve read so far, and what I recall from the last time I read part of the book(s) is that the analysis is very exact and all coefficients and constants are carefully computed. Given how unequal different operations are (a multiply vs a read from L2 cache vs a network round trip), it seems that counting operations is probably not worth the trouble in almost all cases. It will be interesting to see how Big O is used throughout the book.</summary></entry><entry><title type="html">section-1.2.10</title><link href="/taobataocp/taobataocp/2021/01/02/section-1.2.10.html" rel="alternate" type="text/html" title="section-1.2.10" /><published>2021-01-02T22:26:16+00:00</published><updated>2021-01-02T22:26:16+00:00</updated><id>/taobataocp/taobataocp/2021/01/02/section-1.2.10</id><content type="html" xml:base="/taobataocp/taobataocp/2021/01/02/section-1.2.10.html">&lt;p&gt;This section is a doozy.&lt;/p&gt;

&lt;p&gt;The first chunk is about analyzing the running time of a simple algorithm to find the max of an array.  For some reason, he iterates through the array backwards, maybe so that the termination check is k==0 which is “nice?”  Also, the algorithm is written using “goto” form.&lt;/p&gt;

&lt;p&gt;Something that I remember sticking out from when I read this section years ago is that when you draw the algorithm as a flow chart, the number of “incoming edges” to a step has to be the same as the number of “outgoing edges” which he relates to Kirchoff’s current law from electronics.&lt;/p&gt;

&lt;p&gt;The next chunk is about analyizing how many times the max value is updated in the loop, assuming that the array contains no duplicates and each permutation is equally likely.  I was sort of surprised that the average number of swaps when the array is length 3 is 5/6 (less than 1).  You have to swap at least once for 2/3 of the permutations, but the #swaps/#permutations is less than 1 which still seems weird.&lt;/p&gt;

&lt;p&gt;The next part uses a generating function on probablities to compute the expected number of swaps and its variance.  There’s some cleverness that he does by taking derivatives of the generating function to get nice expressions for the mean/varaiance.&lt;/p&gt;

&lt;p&gt;I don’t think I ever used generating functions for analysis, even in my grad level algorithms class at UBC, so it’s an interesting perspective for analysis.  It feels like the stuff we studied was simple enough that we didn’t need that tool?&lt;/p&gt;</content><author><name></name></author><category term="taobataocp" /><summary type="html">This section is a doozy.</summary></entry><entry><title type="html">section-1.2.9</title><link href="/taobataocp/taobataocp/2021/01/01/section-1.2.9.html" rel="alternate" type="text/html" title="section-1.2.9" /><published>2021-01-01T03:08:51+00:00</published><updated>2021-01-01T03:08:51+00:00</updated><id>/taobataocp/taobataocp/2021/01/01/section-1.2.9</id><content type="html" xml:base="/taobataocp/taobataocp/2021/01/01/section-1.2.9.html">&lt;p&gt;This section is about generating functions which take a sequence and turn it into a function.  The most interesting nugget piece was talking about how it doesn’t really matter if the generating function converges.  You use them as a tool to get some insight into understanding a sequence and once you have an idea what the answer should be (a closed form representatino or whatever), you can prove it another way (like by using induction).  I think the DEA must have read this chapter when they went all in on “parallel construction.”&lt;/p&gt;

&lt;p&gt;This is another section that I mostly zoned out for most of.&lt;/p&gt;</content><author><name></name></author><category term="taobataocp" /><summary type="html">This section is about generating functions which take a sequence and turn it into a function. The most interesting nugget piece was talking about how it doesn’t really matter if the generating function converges. You use them as a tool to get some insight into understanding a sequence and once you have an idea what the answer should be (a closed form representatino or whatever), you can prove it another way (like by using induction). I think the DEA must have read this chapter when they went all in on “parallel construction.”</summary></entry><entry><title type="html">section-1.2.8</title><link href="/taobataocp/taobataocp/2020/12/30/section-1.2.8.html" rel="alternate" type="text/html" title="section-1.2.8" /><published>2020-12-30T03:36:15+00:00</published><updated>2020-12-30T03:36:15+00:00</updated><id>/taobataocp/taobataocp/2020/12/30/section-1.2.8</id><content type="html" xml:base="/taobataocp/taobataocp/2020/12/30/section-1.2.8.html">&lt;p&gt;This section is about Fibonacci numbers.  There’s a long discussion of the relationship between Fibonacci numbers and Euclid’s algorithm with a proof that gcd(F_m, F_n) = F_gcd(m_n) which is sort of interesting.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Then he uses a generating function to come up with a closed form formula for F_n using phi.  One piece that confused me was that he said “if G(z) exists” which I suppose means “assuming&lt;/td&gt;
      &lt;td&gt;z&lt;/td&gt;
      &lt;td&gt;&amp;lt; 1” so that G has a finite value.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In any case, I’m ready to be finished with all the mathematical preliminaries and get onto the MIX stuff.&lt;/p&gt;</content><author><name></name></author><category term="taobataocp" /><summary type="html">This section is about Fibonacci numbers. There’s a long discussion of the relationship between Fibonacci numbers and Euclid’s algorithm with a proof that gcd(F_m, F_n) = F_gcd(m_n) which is sort of interesting.</summary></entry><entry><title type="html">section-1.2.7</title><link href="/taobataocp/taobataocp/2020/12/29/section-1.2.7.html" rel="alternate" type="text/html" title="section-1.2.7" /><published>2020-12-29T04:50:23+00:00</published><updated>2020-12-29T04:50:23+00:00</updated><id>/taobataocp/taobataocp/2020/12/29/section-1.2.7</id><content type="html" xml:base="/taobataocp/taobataocp/2020/12/29/section-1.2.7.html">&lt;p&gt;This section is about the harmonic numbers (sum_k 1/k).  There’s an interesting formula for H_n which is natural log of n + some small fudge factor terms.  I remember proving that the harmonic series diverges by computing the integral of 1/x in calc class, but don’t remember hearing the connection that H_n was very very close to ln(n).&lt;/p&gt;

&lt;p&gt;The rest of the sections was some maniuplations of sums combining harmonic numbers with binomial coefficients… I lost interest and skimmed over it.&lt;/p&gt;</content><author><name></name></author><category term="taobataocp" /><summary type="html">This section is about the harmonic numbers (sum_k 1/k). There’s an interesting formula for H_n which is natural log of n + some small fudge factor terms. I remember proving that the harmonic series diverges by computing the integral of 1/x in calc class, but don’t remember hearing the connection that H_n was very very close to ln(n).</summary></entry><entry><title type="html">section-1.2.6</title><link href="/taobataocp/taobataocp/2020/12/18/section-1.2.6.html" rel="alternate" type="text/html" title="section-1.2.6" /><published>2020-12-18T06:38:20+00:00</published><updated>2020-12-18T06:38:20+00:00</updated><id>/taobataocp/taobataocp/2020/12/18/section-1.2.6</id><content type="html" xml:base="/taobataocp/taobataocp/2020/12/18/section-1.2.6.html">&lt;p&gt;This section is about binomial coefficients and their identities.  It’s the longest section by far so far and my eyes glazed over.  There was one explanation of an identity that caught my attention:&lt;/p&gt;

&lt;p&gt;r + s choose n means “choose n people from r men and s women” which is equal to sum_k r choose k * s choose n -k.  The right side is choose r of the men and s of the women.&lt;/p&gt;

&lt;p&gt;It ends with a discussion of Stirling numbers which I don’t think I’ve heard of, and wasn’t interested in.&lt;/p&gt;</content><author><name></name></author><category term="taobataocp" /><summary type="html">This section is about binomial coefficients and their identities. It’s the longest section by far so far and my eyes glazed over. There was one explanation of an identity that caught my attention:</summary></entry></feed>